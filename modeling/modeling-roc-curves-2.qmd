---
title: "modeling-roc-curves"
format: html
---

## load packages
```{r}
library(tidyverse) 
library(rsample)
library(broom)
library(yardstick)
library(janitor)
library(dplyr)
library(ggplot2)
library(stringr)
library(tidymodels)
library(ranger)
library(probably)
library(vip)
library(rpart.plot)
library(e1071)
library(pROC)
library(cowplot)
```

## data
```{r}
uds <- read_csv("C:/Users/new account/Desktop/NACC_data/original_data/data_cleaned/uds.csv")

# select predictors
predictors_df <- uds %>% 
  select(SEX,
          EDUC,
          NACCAGE,
          VEG,
          ANIMALS,
          TRAILA,
          TRAILB,
          CRAFTDRE,
          MINTTOTS,
          MEMPROB,
          DROPACT,
          WRTHLESS,
          BETTER,
          BORED,
          HELPLESS,
          TAXES,
          BILLS,
          REMDATES,
          TRAVEL,
          NACCUDSD,
          MEALPREP,
          GAMES) %>%
  mutate(across(c(SEX,
                  MEMPROB,
                  DROPACT,
                  WRTHLESS,
                  BETTER,
                  BORED,
                  HELPLESS,
                  TAXES,
                  BILLS,
                  REMDATES,
                  TRAVEL,
                  MEALPREP,
                  GAMES), as.factor))

#create 2 seperate dataset with just MCI vs AD and HC vs Not HC  
#(0 and 1s are for the logistic regression Model)
HC_notHC_set <- predictors_df |>
  filter(NACCUDSD == "1" | NACCUDSD == "3" | NACCUDSD == "4") |>
  mutate(cog_stat = case_when(NACCUDSD == 1 ~ "0",          #HC = 0
                              NACCUDSD == 3 ~ "1",      #Not HC = 1
                              NACCUDSD == 4 ~ "1")) |> na.omit() 

MCI_AD_set <- predictors_df |>
  filter(NACCUDSD == "3" | NACCUDSD == "4") |>
  mutate(cog_stat = case_when(NACCUDSD == 3 ~ "0",         #MCI = 0, AD = 1
                              NACCUDSD == 4 ~ "1")) |> na.omit() 

HC_notHC_set <- HC_notHC_set |>
   mutate(
    cog_stat = cog_stat |> 
      as.factor() |>
      relevel(ref = "0")
  )
MCI_AD_set <- MCI_AD_set |>
   mutate(
    cog_stat = cog_stat |> 
      as.factor() |>
      relevel(ref = "0")
   )
```

## logistic regression
```{r}
# splitting training and holdout for both data sets
set.seed(236967)
HC_notHC_set_split <- initial_split(
  HC_notHC_set,
  prop = 0.8 
)

HC_notHC_set_training <- training(HC_notHC_set_split)
HC_notHC_set_holdout <- testing(HC_notHC_set_split)

set.seed(236967)
MCI_AD_set_split <- initial_split(
  MCI_AD_set,
  prop = 0.8 
)

MCI_AD_set_training <- training(MCI_AD_set_split)
MCI_AD_set_holdout <- testing(MCI_AD_set_split)

########### model
HC_notHC_logistic_1 <- glm(
  cog_stat ~ SEX + EDUC + NACCAGE + VEG + ANIMALS + TRAILA + TRAILB + CRAFTDRE + MINTTOTS + MEMPROB + DROPACT + WRTHLESS + BETTER + BORED + HELPLESS + TAXES + BILLS + REMDATES + MEALPREP + GAMES , 
  data = HC_notHC_set_training,
  family = "binomial")
#sometimes do this at the very end
HC_notHC_logistic_1 |>
  tidy()

MCI_AD_logistic_1 <- glm(
  cog_stat ~ SEX + EDUC + NACCAGE + VEG + ANIMALS + TRAILA + TRAILB + CRAFTDRE + MINTTOTS  + MEMPROB + DROPACT + WRTHLESS + BETTER + BORED + HELPLESS + TAXES + BILLS + REMDATES + MEALPREP + GAMES, 
  data = MCI_AD_set_training,
  family = "binomial")
#sometimes do this at the very end
MCI_AD_logistic_1 |>
  tidy()

########### predictions
HC_notHC_logr1_pred <- HC_notHC_logistic_1 |>
  augment(newdata = HC_notHC_set_holdout,
          type.predict = "response")
HC_notHC_logr1_predictions <- HC_notHC_logr1_pred |>
  mutate(
    predicted_class = if_else(
      .fitted > 0.5, # more likely to be not HC than HC, can adjust 
      "1", # Not HC
      "0" # HC
      ) |>
      as.factor() |> # convert to factor
      relevel(ref = "0") # define reference level
    )

MCI_AD_logr1_pred <- MCI_AD_logistic_1 |>
  augment(newdata = MCI_AD_set_holdout,
          type.predict = "response")
MCI_AD_logr1_predictions <- MCI_AD_logr1_pred |>
  mutate(
    predicted_class = if_else(
      .fitted > 0.5, # more likely to be AD than MCI, can adjust 
      "1", # AD
      "0" # MCI
      ) |>
      as.factor() |> # convert to factor
      relevel(ref = "0") # define reference level
    )

########### confusion matrix
HC_notHC_logr1_predictions |>
  conf_mat(
    truth = cog_stat, # row variable
    estimate = predicted_class # column variable
    )

MCI_AD_logr1_predictions |>
  conf_mat(
    truth = cog_stat, # row variable
    estimate = predicted_class # column variable
    )
```
## ROC curve for logistic regression (just MCI vs. AD classification)
```{r}
MCI_AD_logr1_roc_curve <- MCI_AD_logr1_predictions |>
  roc_curve(truth = cog_stat,
            .fitted,
            event_level = "second")

MCI_AD_logr1_roc_curve |> head(4)

# plotting ROC curve
p1 <- autoplot(MCI_AD_logr1_roc_curve) +
  theme(
    axis.title = element_text(size = 10),
    axis.text = element_text(size = 10)
  )
p1

# AUC
MCI_AD_logr1_predictions |>
  roc_auc(truth = cog_stat,
            .fitted,
            event_level = "second")
```
## random forest (just MCI vs. AD classification)
```{r}
# Training and Test Set
set.seed(777)
udscog3_split <- initial_split(MCI_AD_set, prop = 0.8)
udscog3_train <- training(udscog3_split)
udscog3_test <- testing(udscog3_split)


########################
# Classification Trees
########################

# tree-tidy model
# treeR_model <- decision_tree(mode = "regression", engine = "rpart",
#                            cost_complexity = tune())
# let's just tune the cost-complexity parameter
# can be used for CDRSUM AND MMSE below this chunk


# treeC-tidy model
#treeC_model <- set_mode(treeR_model, "classification")

# Equivalent to:
treeC_model3 <- decision_tree(mode = "classification", engine = "rpart", cost_complexity = tune())
# because we are still using rpart and tuning the cost-complexity parameter


# treeC-tidy recipe AD}
treeC_recipe_AD3 <- recipe(cog_stat ~ . ,data = udscog3_train)

treeC_wflow_AD3 <- workflow() |>
  add_model(treeC_model3) |>
  add_recipe(treeC_recipe_AD3)


# tune Cmodel kfold 1 AD
set.seed(777)
AD_kfold3 <- vfold_cv(udscog3_train, v = 5, repeats = 3) 

# changed mn_log_loss to auc_roc
treeC_tune3 <- tune_grid(treeC_model3, 
                         treeC_recipe_AD3, 
                         resamples = AD_kfold3, 
                         metrics = metric_set(roc_auc),
                         grid = grid_regular(cost_complexity(range = c(-2, 0)), levels = 10))

autoplot(treeC_tune3)

# changed mn_log_loss to roc_auc
treeC_best3 <- select_by_one_std_err(
  treeC_tune3,
  metric = "roc_auc",
  desc(cost_complexity)
)
treeC_best3


# fit treeC an actual tiny tree
treeC_wflow3_final <- finalize_workflow(treeC_wflow_AD3, parameters = treeC_best3) 

treeC_fit3 <- fit(treeC_wflow3_final, data = udscog3_train)

rpart.plot(extract_fit_engine(treeC_fit3), main = "Decision Tree Plot", type = 1, extra = 100, under = TRUE, cex = 0.6,
           roundint =FALSE)


# extract_fit_engine(treeC_fit2) %>%
#   plot()


### Bagging and Random Forests for Classification
# rfC-tidy model
rfC_model3 <- rand_forest(mode = "classification", engine = "ranger") |>
  set_args(seed = 777,
           importance = "permutation",
           mtry = tune()
  )

rfC_recipe_AD3 <- recipe(
  cog_stat ~ .,
  data = udscog3_train
)


rfC_wflow_AD3 <- workflow() |>
  add_model(rfC_model3) |>
  add_recipe(rfC_recipe_AD3)


# tune model kfold rfC}
# I'm sure there's a better way, but this works
n_predictorsC3 <- sum(rfC_recipe_AD3$var_info$role == "predictor")
manual_gridC3 <- expand.grid(mtry = seq(1, n_predictorsC3))

# changed mn_log_loss to auc_roc
rfC_tune3 <- tune_grid(rfC_model3, 
                       rfC_recipe_AD3, 
                       resamples = AD_kfold3, 
                       metrics = metric_set(roc_auc, accuracy),
                       grid = manual_gridC3)

# took 20 minutes to run
#autoplot(rfC_tune3)

rfC_best <- select_best(
  rfC_tune3,
  metric = "roc_auc"
)
rfC_wflow_final3 <- finalize_workflow(rfC_wflow_AD3, parameters = rfC_best) 
rfC_fit3 <- fit(rfC_wflow_final3, data = udscog3_train)
rfC_multi_AD3 <- broom::augment(rfC_fit3, new_data = udscog3_test)

# roc plot not working
p2 <- roc_auc(rfC_multi_AD3,
        truth = cog_stat,
        .pred_class)
p2
```

## SVM (just MCI vs. AD classification)
```{r}
# training and test set for MCI_AD_set
set.seed(777)
MCI_AD_split <- initial_split(MCI_AD_set, prop = 0.8)
MCI_AD_train <- training(MCI_AD_split)
MCI_AD_test <- testing(MCI_AD_split)

svmfit2 <- svm(factor(cog_stat) ~ ., data = MCI_AD_train, kernel = "linear", cost = 10, cross = 10)
summary(svmfit2)

pred2 <- predict(svmfit2, MCI_AD_test)
plot(pred2)
table(pred2, MCI_AD_test$cog_stat)

actual <- factor(MCI_AD_test$cog_stat, levels = c("MCI", "AD"))
roc_curve <- roc(actual, as.numeric(pred2))
auc_value <- auc(roc_curve)
print(auc_value)
plot(roc_curve, col = "red")

# not working
p3 <- roc_curve(pred2, MCI_AD_test$cog_stat) %>%
  ggplot(aes(x = 1 specificity, y = sensitivity)) + 
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  theme_bw()


```

## plotting ROC curves together
```{r}
# p2 (the random forest ROC isn't working)
# p3 (roc for svm also isn't working)
plot_grid(p1, p2, p3, labels = c("logistic regression", "random forest", "SVM"), 
          label_size = 10, ncol=3)
```
