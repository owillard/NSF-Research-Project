---
title: "feature-importance-plot"
format: html
---

## load necessary libraries
```{r}
library(dplyr)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(vip)
library(rpart.plot)
```

## read data
```{r}
mri <- read.csv("mri.csv")
mri_raw <- read.csv("investigator_mri_nacc57.csv")
uds <- read.csv("uds.csv")
# Select relevant variables from UDS dataset
uds_selected <- uds %>% 
  select(
    SEX, EDUC, NACCAGE, VEG, ANIMALS, TRAILA, TRAILB, CRAFTDRE, MINTTOTS, 
    MEMPROB, DROPACT, WRTHLESS, BETTER, BORED, HELPLESS, TAXES, BILLS, 
    REMDATES, TRAVEL, MEALPREP, GAMES
  )

# Create cognitive status factor
uds <- uds %>%
  mutate(
    cog_status = factor(NACCUDSD, levels = c(1, 2, 3, 4), labels = c("HC", "Impaired", "MCI", "AD")),
    cog_hc_or_not = factor(NACCUDSD, levels = c(1, 2, 3, 4), labels = c("HC", "NHC", "NHC", "NHC"))
  )

# Define factor variables
factor_vars <- c("TAXES", "BILLS", "REMDATES", "TRAVEL", "MEMPROB", 
                 "DROPACT", "WRTHLESS", "BETTER", "BORED", "HELPLESS", "SEX",
                 "MEALPREP", "GAMES"
)

# Filter data for MCI and AD only and select relevant columns
uds_mci_ad_only <- uds %>%
  select(
    SEX, EDUC, NACCAGE, VEG, ANIMALS, TRAILA, TRAILB, CRAFTDRE, MINTTOTS, MEMPROB,
    DROPACT, WRTHLESS, BETTER, BORED, HELPLESS, TAXES, BILLS, REMDATES, TRAVEL, MEALPREP, GAMES, NACCUDSD
  ) %>%
  filter(NACCUDSD == 3 | NACCUDSD == 4) %>%
  mutate(across(all_of(factor_vars), factor)) %>%
  mutate(cog_status = factor(NACCUDSD, levels = c(3, 4), labels = c("MCI", "AD"))) %>%
  select(-NACCUDSD) %>%
  na.omit()

# Split data into training and test sets
set.seed(777)
udscog3_split <- initial_split(uds_mci_ad_only, prop = 0.8)
udscog3_train <- training(udscog3_split)
udscog3_test <- testing(udscog3_split)
```

## tree
```{r}
# Example graph: Distribution of cognitive status by sex
ggplot(uds_mci_ad_only, aes(x = SEX, fill = cog_status)) +
  geom_bar(position = "dodge") +
  labs(
    title = "Distribution of Cognitive Status by Sex",
    x = "Sex",
    y = "Count",
    fill = "Cognitive Status"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title.x = element_text(size = 14, face = "bold"),
    axis.title.y = element_text(size = 14, face = "bold"),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    legend.title = element_text(size = 14),
    legend.text = element_text(size = 12)
  )



# tree-tidy model
# treeR_model <- decision_tree(mode = "regression", engine = "rpart",
#                            cost_complexity = tune())
# let's just tune the cost-complexity parameter
# can be used for CDRSUM AND MMSE below this chunk


# treeC-tidy model
#treeC_model <- set_mode(treeR_model, "classification")

# Equivalent to:
treeC_model3 <- decision_tree(mode = "classification", engine = "rpart", cost_complexity = tune())
# because we are still using rpart and tuning the cost-complexity parameter


# treeC-tidy recipe AD}
treeC_recipe_AD3 <- recipe(cog_status ~ . ,data = udscog3_train)

treeC_wflow_AD3 <- workflow() |>
  add_model(treeC_model3) |>
  add_recipe(treeC_recipe_AD3)


# tune Cmodel kfold 1 AD
set.seed(777)
AD_kfold3 <- vfold_cv(udscog3_train, v = 5, repeats = 3) 

# changed mn_log_loss to auc_roc
treeC_tune3 <- tune_grid(treeC_model3, 
                         treeC_recipe_AD3, 
                         resamples = AD_kfold3, 
                         metrics = metric_set(roc_auc),
                         grid = grid_regular(cost_complexity(range = c(-2, 0)), levels = 10))

autoplot(treeC_tune3)

# changed mn_log_loss to roc_auc
treeC_best3 <- select_by_one_std_err(
  treeC_tune3,
  metric = "roc_auc",
  desc(cost_complexity)
)
treeC_best3


# fit treeC an actual tiny tree
treeC_wflow3_final <- finalize_workflow(treeC_wflow_AD3, parameters = treeC_best3) 

treeC_fit3 <- fit(treeC_wflow3_final, data = udscog3_train)

rpart.plot(extract_fit_engine(treeC_fit3), main = "Decision Tree Plot", type = 1, extra = 100, under = TRUE, cex = 0.6,
           roundint =FALSE)


# extract_fit_engine(treeC_fit2) %>%
#   plot()


### Bagging and Random Forests for Classification
# rfC-tidy model
rfC_model3 <- rand_forest(mode = "classification", engine = "ranger") |>
  set_args(seed = 777,
           importance = "permutation",
           mtry = tune()
  )

rfC_recipe_AD3 <- recipe(
  cog_status ~ .,
  data = udscog3_train
)


rfC_wflow_AD3 <- workflow() |>
  add_model(rfC_model3) |>
  add_recipe(rfC_recipe_AD3)


# tune model kfold rfC}
# I'm sure there's a better way, but this works
n_predictorsC3 <- sum(rfC_recipe_AD3$var_info$role == "predictor")
manual_gridC3 <- expand.grid(mtry = seq(1, n_predictorsC3))





rfC_tune3 <- tune_grid(rfC_model3, 
                       rfC_recipe_AD3, 
                       resamples = AD_kfold3, 
                       metrics = metric_set(roc_auc, accuracy),
                       grid = manual_gridC3)

rfC_best <- select_best(
  rfC_tune3,
  metric = "roc_auc"
)

rfC_recipe_AD3 <- recipe(
  cog_status ~ .,
  data = udscog3_train
)

rfC_best <- select_best(
  rfC_tune3,
  metric = "roc_auc"
)

n_predictorsC3 <- sum(rfC_recipe_AD3$var_info$role == "predictor")


manual_gridC3 <- expand.grid(mtry = seq(1, n_predictorsC3))


rfC_tune3 <- tune_grid(rfC_model3, 
                       rfC_recipe_AD3, 
                       resamples = AD_kfold3, 
                       metrics = metric_set(roc_auc, accuracy),
                       grid = manual_gridC3)


rfC_wflow_AD3 <- workflow() |>
  add_model(rfC_model3) |>
  add_recipe(rfC_recipe_AD3)

rfC_wflow_final3 <- finalize_workflow(rfC_wflow_AD3, parameters = rfC_best) 
rfC_fit3 <- fit(rfC_wflow_final3, data = udscog3_train)
rfC_fit3

# fit rfC-tidy model
rfC_wflow_final3 <- finalize_workflow(rfC_wflow_AD3, parameters = rfC_best) 
rfC_fit3 <- fit(rfC_wflow_final3, data = udscog3_train)
rfC_fit3

rfC_model3 <- rand_forest(mode = "classification", engine = "ranger") |>
  set_args(seed = 777,
           importance = "permutation",
           mtry = tune()
  )



rfC_engine3 <- rfC_fit3 |> extract_fit_engine()
rfC_engine3
#extract_fit_engine(treeC_fit2) |>
#plot(
#ylim = c(-0.2, 1.2))
#extract_fit_engine(treeC_fit2) |>
#text(cex = 0.5)

rfC_engine3 |> pluck("prediction.error")

vip_obj <- vip(rfC_engine3, num_features = 20, geom = "point", aesthetics = list(color = "blue", size = 3))
```

## VIP
```{r}
# Extract the data from the vip object
vip_data <- vip_obj$data

vip_plot +
  ggtitle("Variable Importance Plot") +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold", color = "darkblue"),
    axis.title.x = element_text(size = 14, face = "bold", color = "darkred"),
    axis.title.y = element_text(size = 14, face = "bold", color = "darkred"),
    axis.text.x = element_text(size = 12, color = "black"),
    axis.text.y = element_text(size = 12, color = "black")
  ) +
  xlab("Variables") +
  ylab("Importance") +
  coord_flip() + # Flip coordinates for better readability
  scale_fill_manual(values = c("#4B0082", "violet", "#008080")) + # Customize colors
  guides(fill = guide_legend(title = "Variables")) # Customize legend


vip_data_top10 <- vip_data %>% 
  arrange(desc(Importance)) %>% 
  head(10)

vip_data_top10$Importance <- (vip_data_top10$Importance / max(vip_data_top10$Importance)) * 100


# Create the plot with ggplot2
ggplot(vip_data_top10, aes(x = reorder(Variable, Importance), y = Importance)) +
  geom_col(aes(fill = Variable)) +
  coord_flip() + # Flip coordinates for better readability
  scale_fill_manual(values = rep(c("#4A0082"), length.out = nrow(vip_data_top10))) + # Customize colors
  labs(
    title = "Top 10 Variable Importance Plot",
    x = "Variables",
    y = "Importance (0-100)"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, size = 24, face = "bold", color = "black"),
    axis.title.x = element_text(size = 20, face = "bold", color = "black"),
    axis.title.y = element_text(size = 20, face = "bold", color = "black"),
    axis.text.x = element_text(size = 16, color = "black"),
    axis.text.y = element_text(size = 16, color = "black"),
    legend.position = "none" # Hide legend
  ) +
  scale_y_continuous(limits = c(0, 100))
```
